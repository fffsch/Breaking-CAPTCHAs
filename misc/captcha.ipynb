{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b76d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import ops\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "data_dir = Path(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b2ad095",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m images = \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m(\u001b[43mdata_dir\u001b[49m.glob(\u001b[33m\"\u001b[39m\u001b[33m*.png\u001b[39m\u001b[33m\"\u001b[39m)))))\n\u001b[32m      2\u001b[39m labels = [img.split(os.path.sep)[-\u001b[32m1\u001b[39m].split(\u001b[33m\"\u001b[39m\u001b[33m-0.png\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[32m      4\u001b[39m characters = \u001b[38;5;28mset\u001b[39m(char \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m label)\n",
      "\u001b[31mNameError\u001b[39m: name 'data_dir' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "images = sorted(list(map(str, list(data_dir.glob(\"*.png\")))))\n",
    "labels = [img.split(os.path.sep)[-1].split(\"-0.png\")[0] for img in images]\n",
    "\n",
    "characters = set(char for label in labels for char in label)\n",
    "characters = sorted(list(characters))\n",
    "\n",
    "batchsize = 16\n",
    "\n",
    "img_width = 200\n",
    "img_height = 50\n",
    "\n",
    "downsample_factor = 4\n",
    "    \n",
    "len_offset = 0\n",
    "\n",
    "max_len = max([len(label) for label in labels]) + len_offset\n",
    "\n",
    "\n",
    "char_num = layers.StringLookup(vocabulary=characters, mask_token=\"\")\n",
    "num_char = layers.StringLookup(vocabulary=char_num.get_vocabulary(), mask_token=\"\", invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47fe251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(images, labels, train_size=0.9, shuffle=True):\n",
    "    size = len(images)\n",
    "\n",
    "    indices = ops.arange(size)\n",
    "    if shuffle:\n",
    "        indices = keras.random.shuffle(indices)\n",
    "\n",
    "    train_samples = int(size * train_size)\n",
    "\n",
    "    x_train, y_train = images[indices[:train_samples]], labels[indices[:train_samples]]\n",
    "    x_valid, y_valid = images[indices[train_samples:]], labels[indices[train_samples:]]\n",
    "    return x_train, x_valid, y_train, y_valid\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = split_data(np.array(images), np.array(labels))\n",
    "\n",
    "\n",
    "def encode_single_sample(img_path, label):\n",
    "    img = tf.io.read_file(img_path)\n",
    "\n",
    "    img = tf.io.decode_png(img, channels=1)\n",
    "\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "    img = ops.image.resize(img, [img_height, img_width])\n",
    "\n",
    "    img = ops.transpose(img, axes=[1, 0, 2])\n",
    "\n",
    "    label = char_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n",
    "\n",
    "    return {\"image\": img, \"label\": label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f95c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the shapes for padding. This structure MUST match\n",
    "# the dictionary returned by encode_single_sample.\n",
    "padded_shapes = {\n",
    "    \"image\": [img_width, img_height, 1],  # After transpose, shape is [img_y, img_x, 1]\n",
    "    \"label\": [None],             # This is the key: pad the label's 1st dimension\n",
    "}\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = (\n",
    "    train_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # --- FIX IS HERE ---\n",
    "    .padded_batch(batchsize, padded_shapes=padded_shapes)\n",
    "    # -------------------\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n",
    "validation_dataset = (\n",
    "    validation_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # --- FIX IS HERE ---\n",
    "    .padded_batch(batchsize, padded_shapes=padded_shapes)\n",
    "    # -------------------\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c696bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(4, 4, figsize=(10, 5))\n",
    "for batch in train_dataset.take(1):\n",
    "    images = batch[\"image\"]\n",
    "    labels = batch[\"label\"]\n",
    "    for i in range(16):\n",
    "        img = (images[i] * 255).numpy().astype(\"uint8\")\n",
    "        label = tf.strings.reduce_join(num_char(labels[i])).numpy().decode(\"utf-8\")\n",
    "        ax[i // 4, i % 4].imshow(img[:, :, 0].T, cmap=\"gray\")\n",
    "        ax[i // 4, i % 4].set_title(label)\n",
    "        ax[i // 4, i % 4].axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Create a single subplot. 'ax' is now a single Axes object.\n",
    "# A figsize that is wider than it is tall is usually good for a CAPTCHA.\n",
    "_, ax = plt.subplots(figsize=(7, 2)) \n",
    "\n",
    "for batch in train_dataset.take(1):\n",
    "    images = batch[\"image\"]\n",
    "    labels = batch[\"label\"]\n",
    "    \n",
    "    # --- Plot only the first item (index 0) ---\n",
    "    img = (images[0] * 255).numpy().astype(\"uint8\")\n",
    "    label = tf.strings.reduce_join(num_char(labels[0])).numpy().decode(\"utf-8\")\n",
    "    print(label)\n",
    "    # Plot on the single 'ax'\n",
    "    ax.imshow(img[:, :, 0].T, cmap=\"gray\")\n",
    "    ax.set_title(label)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6d5c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n",
    "    label_length = ops.cast(ops.squeeze(label_length, axis=-1), dtype=\"int32\")\n",
    "    input_length = ops.cast(ops.squeeze(input_length, axis=-1), dtype=\"int32\")\n",
    "    sparse_labels = ops.cast(\n",
    "        ctc_label_dense_to_sparse(y_true, label_length), dtype=\"int32\"\n",
    "    )\n",
    "\n",
    "    y_pred = ops.log(ops.transpose(y_pred, axes=[1, 0, 2]) + keras.backend.epsilon())\n",
    "\n",
    "    return ops.expand_dims(\n",
    "        tf.compat.v1.nn.ctc_loss(\n",
    "            inputs=y_pred, labels=sparse_labels, sequence_length=input_length\n",
    "        ),\n",
    "        1,\n",
    "    )\n",
    "\n",
    "\n",
    "def ctc_label_dense_to_sparse(labels, label_lengths):\n",
    "    label_shape = ops.shape(labels)\n",
    "    num_batches_tns = ops.stack([label_shape[0]])\n",
    "    max_num_labels_tns = ops.stack([label_shape[1]])\n",
    "\n",
    "    def range_less_than(old_input, current_input):\n",
    "        return ops.expand_dims(ops.arange(ops.shape(old_input)[1]), 0) < tf.fill(\n",
    "            max_num_labels_tns, current_input\n",
    "        )\n",
    "\n",
    "    init = ops.cast(tf.fill([1, label_shape[1]], 0), dtype=\"bool\")\n",
    "    dense_mask = tf.compat.v1.scan(\n",
    "        range_less_than, label_lengths, initializer=init, parallel_iterations=1\n",
    "    )\n",
    "    dense_mask = dense_mask[:, 0, :]\n",
    "\n",
    "    label_array = ops.reshape(\n",
    "        ops.tile(ops.arange(0, label_shape[1]), num_batches_tns), label_shape\n",
    "    )\n",
    "    label_ind = tf.compat.v1.boolean_mask(label_array, dense_mask)\n",
    "\n",
    "    batch_array = ops.transpose(\n",
    "        ops.reshape(\n",
    "            ops.tile(ops.arange(0, label_shape[0]), max_num_labels_tns),\n",
    "            tf.reverse(label_shape, [0]),\n",
    "        )\n",
    "    )\n",
    "    batch_ind = tf.compat.v1.boolean_mask(batch_array, dense_mask)\n",
    "    indices = ops.transpose(\n",
    "        ops.reshape(ops.concatenate([batch_ind, label_ind], axis=0), [2, -1])\n",
    "    )\n",
    "\n",
    "    vals_sparse = tf.compat.v1.gather_nd(labels, indices)\n",
    "\n",
    "    return tf.SparseTensor(\n",
    "        ops.cast(indices, dtype=\"int64\"), \n",
    "        vals_sparse, \n",
    "        ops.cast(label_shape, dtype=\"int64\")\n",
    "    )\n",
    "\n",
    "\n",
    "class CTCLayer(layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = ctc_batch_cost\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Compute the training-time loss value and add it\n",
    "        # to the layer using `self.add_loss()`.\n",
    "        batch_len = ops.cast(ops.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = ops.cast(ops.shape(y_pred)[1], dtype=\"int64\")\n",
    "        label_length = ops.cast(ops.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "        input_length = input_length * ops.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        label_length = label_length * ops.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "        self.add_loss(loss)\n",
    "\n",
    "        # At test time, just return the computed predictions\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6970adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # Inputs to the model\n",
    "    input_img = layers.Input(\n",
    "        shape=(img_width, img_height, 1), name=\"image\", dtype=\"float32\"\n",
    "    )\n",
    "    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n",
    "\n",
    "    # First conv block\n",
    "    x = layers.Conv2D(\n",
    "        32,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\",\n",
    "        name=\"Conv1\",\n",
    "    )(input_img)\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n",
    "\n",
    "    # Second conv block\n",
    "    x = layers.Conv2D(\n",
    "        64,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\",\n",
    "        name=\"Conv2\",\n",
    "    )(x)\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n",
    "\n",
    "    # We have used two max pool with pool size and strides 2.\n",
    "    # Hence, downsampled feature maps are 4x smaller. The number of\n",
    "    # filters in the last layer is 64. Reshape accordingly before\n",
    "    # passing the output to the RNN part of the model\n",
    "    new_shape = ((img_width // 4), (img_height // 4) * 64)\n",
    "    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    # RNNs\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n",
    "\n",
    "    # Output layer\n",
    "    x = layers.Dense(\n",
    "        len(char_num.get_vocabulary()) + 1, activation=\"softmax\", name=\"dense2\"\n",
    "    )(x)\n",
    "\n",
    "    # Add CTC layer for calculating CTC loss at each step\n",
    "    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.models.Model(\n",
    "        inputs=[input_img, labels], outputs=output, name=\"ocr_model_v1\"\n",
    "    )\n",
    "    # Optimizer\n",
    "    opt = keras.optimizers.Adam()\n",
    "    # Compile the model and return\n",
    "    model.compile(optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eedd4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model\n",
    "model = build_model()\n",
    "model.summary()\n",
    "\n",
    "epochs = 100\n",
    "early_stopping_patience = 10\n",
    "# Add early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
