{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57f440f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "057119b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"data/final_yolo_dataset_v3/images/train\"\n",
    "val_path = \"data/final_yolo_dataset_v3/images/val\"\n",
    "\n",
    "#creating character dataset\n",
    "char_dataset_root = \"data/char_dataset\"\n",
    "os.makedirs(char_dataset_root, exist_ok=True)\n",
    "\n",
    "#character labels\n",
    "import string\n",
    "CHARSET = string.ascii_lowercase + string.digits  # 'abcdefghijklmnopqrstuvwxyz0123456789'\n",
    "char_to_idx = {char: idx for idx, char in enumerate(CHARSET)}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "NUM_CLASSES = len(CHARSET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9974b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#segmentation function (preprocessing)\n",
    "def segment_characters(image):\n",
    "    #convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #median blur to reduce salt-and-pepper noise\n",
    "    blurred = cv2.medianBlur(gray, 3)\n",
    "\n",
    "    #adaptive thresholding to get binary image\n",
    "    _, binary = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    #extra\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    opened = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "    dilated = cv2.dilate(opened, kernel, iterations=1)\n",
    "\n",
    "    #find contours\n",
    "    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    #extracting bounding boxes\n",
    "    boxes = []\n",
    "    h_img, w_img = gray.shape\n",
    "\n",
    "    for c in contours:\n",
    "        x, y, w, h = cv2.boundingRect(c)  # bounding rect for contour\n",
    "        area = w * h\n",
    "\n",
    "        # filter small noise boxes and extreme aspect ratios:\n",
    "        if area < 50:            # too small\n",
    "            continue\n",
    "        if h < 10:               # too short\n",
    "            continue\n",
    "        if w/h > 3.5:            # too wide (likely connected characters or lines)\n",
    "            continue\n",
    "\n",
    "        # expand box a little (context)\n",
    "        pad_x = max(1, int(0.05 * w))\n",
    "        pad_y = max(1, int(0.1 * h))\n",
    "        x1 = max(0, x - pad_x)\n",
    "        y1 = max(0, y - pad_y)\n",
    "        x2 = min(w_img, x + w + pad_x)\n",
    "        y2 = min(h_img, y + h + pad_y)\n",
    "\n",
    "        crop = gray[y1:y2, x1:x2]\n",
    "        boxes.append((x1, y1, x2 - x1, y2 - y1, crop))\n",
    "\n",
    "    # sort boxes left->right (by x)\n",
    "    boxes = sorted(boxes, key=lambda b: b[0])\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97a7c816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5600 captchas; unmatched/skipped: 3320\n",
      "Processed 1400 captchas; unmatched/skipped: 828\n"
     ]
    }
   ],
   "source": [
    "def build_char_dataset_from_captchas(captcha_folder, out_root):\n",
    "    \"\"\"\n",
    "    Processes all captcha images in captcha_folder, segments them,\n",
    "    and writes each character crop into folder out_root/<char>/\n",
    "    \"\"\"\n",
    "    os.makedirs(out_root, exist_ok=True)\n",
    "    files = [f for f in os.listdir(captcha_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    unmatched = 0\n",
    "    total = 0\n",
    "\n",
    "    for fname in files:\n",
    "        total += 1\n",
    "        path = os.path.join(captcha_folder, fname)\n",
    "        img = cv2.imread(path)\n",
    "        boxes = segment_characters(img)\n",
    "\n",
    "        # ground-truth label string (part before '-')\n",
    "        label_str = fname.split('-')[0]\n",
    "\n",
    "        # if we got exactly same number of boxes as characters, map 1:1\n",
    "        if len(boxes) == len(label_str):\n",
    "            for i, ch in enumerate(label_str):\n",
    "                _, _, _, _, crop = boxes[i]\n",
    "                # resize small crop to fixed size for classifier\n",
    "                crop_resized = cv2.resize(crop, (28, 28))\n",
    "                # save under out_root/ch/\n",
    "                ch_dir = os.path.join(out_root, ch)\n",
    "                os.makedirs(ch_dir, exist_ok=True)\n",
    "                # name = originalfile_index.png\n",
    "                out_name = os.path.join(ch_dir, f\"{fname.replace('.','_')}_{i}.png\")\n",
    "                cv2.imwrite(out_name, crop_resized)\n",
    "        else:\n",
    "            # heuristic: if more boxes than characters, choose largest N by width (likely characters)\n",
    "            if len(boxes) > len(label_str) and len(label_str) > 0:\n",
    "                # sort boxes by width descending and take top N, then sort those by x ascending\n",
    "                boxes_by_width = sorted(boxes, key=lambda b: b[2], reverse=True)[:len(label_str)]\n",
    "                boxes_by_width = sorted(boxes_by_width, key=lambda b: b[0])\n",
    "                for i, ch in enumerate(label_str):\n",
    "                    _, _, _, _, crop = boxes_by_width[i]\n",
    "                    crop_resized = cv2.resize(crop, (28, 28))\n",
    "                    ch_dir = os.path.join(out_root, ch)\n",
    "                    os.makedirs(ch_dir, exist_ok=True)\n",
    "                    out_name = os.path.join(ch_dir, f\"{fname.replace('.','_')}_{i}.png\")\n",
    "                    cv2.imwrite(out_name, crop_resized)\n",
    "            else:\n",
    "                # if fewer boxes than characters, skip or try splitting wide boxes (skip for now)\n",
    "                unmatched += 1\n",
    "                # optional: save the original captcha to an 'unmatched' folder for manual inspection\n",
    "                os.makedirs(os.path.join(out_root, \"_unmatched\"), exist_ok=True)\n",
    "                cv2.imwrite(os.path.join(out_root, \"_unmatched\", fname), img)\n",
    "\n",
    "    print(f\"Processed {total} captchas; unmatched/skipped: {unmatched}\")\n",
    "\n",
    "# build for train and val\n",
    "build_char_dataset_from_captchas(train_path, os.path.join(char_dataset_root, \"train\"))\n",
    "build_char_dataset_from_captchas(val_path,   os.path.join(char_dataset_root, \"val\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35a02150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train char samples: 13097\n",
      "val char samples: 3296\n"
     ]
    }
   ],
   "source": [
    "class CharFolderDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        # root_dir contains subfolders named by character\n",
    "        for ch in os.listdir(root_dir):\n",
    "            ch_dir = os.path.join(root_dir, ch)\n",
    "            if not os.path.isdir(ch_dir) or ch == \"_unmatched\":\n",
    "                continue\n",
    "            for fname in os.listdir(ch_dir):\n",
    "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.samples.append((os.path.join(ch_dir, fname), ch))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, ch = self.samples[idx]\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = img.astype(\"float32\") / 255.0  # normalize to 0-1\n",
    "        img = np.expand_dims(img, 0)  # (C=1,H,W)\n",
    "        tensor = torch.tensor(img, dtype=torch.float32)\n",
    "        label = char_to_idx[ch]\n",
    "        return tensor, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# create datasets and loaders\n",
    "train_char_root = os.path.join(char_dataset_root, \"train\")\n",
    "val_char_root   = os.path.join(char_dataset_root, \"val\")\n",
    "\n",
    "train_char_ds = CharFolderDataset(train_char_root)\n",
    "val_char_ds   = CharFolderDataset(val_char_root)\n",
    "\n",
    "print(\"train char samples:\", len(train_char_ds))\n",
    "print(\"val char samples:\", len(val_char_ds))\n",
    "\n",
    "train_loader = DataLoader(train_char_ds, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_char_ds, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76af2c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - TrainLoss 3.0793 - ValCharAcc 41.02%\n",
      "Epoch 2/10 - TrainLoss 2.1217 - ValCharAcc 54.40%\n",
      "Epoch 3/10 - TrainLoss 1.8492 - ValCharAcc 57.52%\n",
      "Epoch 4/10 - TrainLoss 1.7120 - ValCharAcc 58.80%\n",
      "Epoch 5/10 - TrainLoss 1.6161 - ValCharAcc 59.28%\n",
      "Epoch 6/10 - TrainLoss 1.5360 - ValCharAcc 61.95%\n",
      "Epoch 7/10 - TrainLoss 1.4704 - ValCharAcc 63.47%\n",
      "Epoch 8/10 - TrainLoss 1.4069 - ValCharAcc 63.29%\n",
      "Epoch 9/10 - TrainLoss 1.3507 - ValCharAcc 64.56%\n",
      "Epoch 10/10 - TrainLoss 1.3112 - ValCharAcc 64.99%\n"
     ]
    }
   ],
   "source": [
    "class CharCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                # 14x14\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                # 7x7\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CharCNN(NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# training loop\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running += loss.item() * imgs.size(0)\n",
    "    train_loss = running / len(train_loader.dataset)\n",
    "\n",
    "    # validation char accuracy\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(imgs)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    val_acc = 100.0 * correct / total if total>0 else 0.0\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - TrainLoss {train_loss:.4f} - ValCharAcc {val_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21c0b469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-level accuracy: 48.45% (3094/6386)\n",
      "Captcha-level accuracy: 10.00% (140/1400)\n"
     ]
    }
   ],
   "source": [
    "def predict_chars_from_crops(crops, model, device):\n",
    "    \"\"\"\n",
    "    crops: list of (crop_gray numpy arrays)\n",
    "    returns: list of predicted characters in order\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for crop in crops:\n",
    "            # resize to same size used in training (28x28)\n",
    "            crop = cv2.resize(crop, (28, 28)).astype(\"float32\") / 255.0\n",
    "            tensor = torch.tensor(crop).unsqueeze(0).unsqueeze(0).to(device)  # (1,1,H,W)\n",
    "            logits = model(tensor)\n",
    "            p = torch.argmax(logits, dim=1).item()\n",
    "            preds.append(idx_to_char[p])\n",
    "    return preds\n",
    "\n",
    "# run over validation captchas\n",
    "total_chars = 0\n",
    "correct_chars = 0\n",
    "total_captchas = 0\n",
    "correct_captchas = 0\n",
    "\n",
    "val_files = [f for f in os.listdir(val_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "for fname in val_files:\n",
    "    total_captchas += 1\n",
    "    path = os.path.join(val_path, fname)\n",
    "    img = cv2.imread(path)\n",
    "    boxes = segment_characters(img)  # list of (x,y,w,h,crop) sorted left->right\n",
    "    crops = [b[4] for b in boxes]\n",
    "\n",
    "    label_str = fname.split('-')[0]\n",
    "\n",
    "    # if counts mismatch, try heuristic to select most likely N crops\n",
    "    if len(crops) != len(label_str):\n",
    "        if len(crops) > len(label_str) and len(label_str) > 0:\n",
    "            # pick largest width crops\n",
    "            boxes_sorted = sorted(boxes, key=lambda b: b[2], reverse=True)[:len(label_str)]\n",
    "            boxes_sorted = sorted(boxes_sorted, key=lambda b: b[0])\n",
    "            crops = [b[4] for b in boxes_sorted]\n",
    "        else:\n",
    "            # skip this captcha for captcha-level accuracy (but include character-level if we can align)\n",
    "            # we'll attempt to align by resizing and predicting anyway, but mark as mismatch if lengths differ\n",
    "            pass\n",
    "\n",
    "    # predict each crop\n",
    "    pred_chars = predict_chars_from_crops(crops, model, device)\n",
    "\n",
    "    # compute character-level stats:\n",
    "    # align by index; only up to min(len(pred), len(gt))\n",
    "    n_compare = min(len(pred_chars), len(label_str))\n",
    "    for i in range(n_compare):\n",
    "        total_chars += 1\n",
    "        if pred_chars[i] == label_str[i]:\n",
    "            correct_chars += 1\n",
    "\n",
    "    # captcha-level: require same length and all equal\n",
    "    if len(pred_chars) == len(label_str) and \"\".join(pred_chars) == label_str:\n",
    "        correct_captchas += 1\n",
    "\n",
    "# results\n",
    "char_acc = 100.0 * correct_chars / total_chars if total_chars>0 else 0.0\n",
    "captcha_acc = 100.0 * correct_captchas / total_captchas if total_captchas>0 else 0.0\n",
    "\n",
    "print(f\"Character-level accuracy: {char_acc:.2f}% ({correct_chars}/{total_chars})\")\n",
    "print(f\"Captcha-level accuracy: {captcha_acc:.2f}% ({correct_captchas}/{total_captchas})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mini_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
